# Deep Reinforcement Learning

## Introduction

The purpose of this code base is to develop and test algorithms in RL and DL space. Reinforcement Learning traced its roots back to dynamic programming and Markov Decision Processes, works developed by Bellman et al back in 1950s. It lost momentum when computational challenges such as curse of dimensionality discouraged large-scale applications. Recently, the rapid development in deep learning has dramatically renewed the view of reinforcement learning. Basically, what Bertsekas and Tsitsiklis wrote in 1990's becomes achievable. We are interested in exploring synergy between RL and DL, with a focus on optimizing sequential decision making problems.

## Open Questions

Here we list some open questions in RL space to motivate our study:

### Bellman Error Minimization

### Practical Bandit
Converence; delayed reward; reward mismatch; correlated bandit; strategy

### Contextual Bandit

### Holistic state-space estimation and optimization

### Basic Reinforcement Learning Architecture
The fundamental RL architecture includes the state, the action, the reward, and the environment (e.g., state transition). At the same time, like Warren Powell pointed out correctly, all the basic concepts are not perfectly defined.

### Forecasting versus Optimization
Supervised learning can be viewed as a prediction problem, with the goal to minimize generalized error on testing dataset. At the same time, Reinforcement Learning is not a prediction problem, but an optimization one. What are the pros and cons when incorporating SL in RL?

### Performance of Stochastic Gradient Descent

### How to unify multiple RL paradigms 
Sutton Barto framework

